# Finding the Root Cause

Bu kavramlarla ilk karşılaştığınızda, bir üretim vakasıyla karşılaştığınızda sorunun temel nedenini zaten biliyormuşsunuz gibi görünebilir. Ancak çoğu zaman bu doğru değildir. Aşırı yüklenmiş sunucu örneğimizde, yedekleme sisteminin web sitelerinin çalışmasını engellediğini fark ettik ve böylece kullanıcının engellemesini kaldırmak için bu acil sorunu hafiflettik. Ancak sunucumuzun takılıp kalmasının temel nedenini gerçekten araştırmadık. Bunun nedeni ağ bant genişliğinin doymuş olması, disk aktarımının çok yavaş olması, sabit sürücünün arızalı olması veya başka birçok neden olabilir. Ayrıca yedeklemelerimizin gelecekte başarılı bir şekilde çalışabilmesini sağlamak için hiçbir şey yapmadık. Kök nedeni anlamak, uzun vadeli iyileştirmeyi gerçekleştirmek için çok önemlidir. Peki sorunun asıl temel nedenini nasıl bulacağız? Genellikle sahip olduğumuz bilgilere bakarak, sorunu açıklayabilecek bir hipotez ortaya çıkararak ve ardından hipotezimizi test ederek bir döngüyü takip ederiz. Teorimizi doğrularsak temel nedeni bulduk. Eğer bunu yapmazsak, en başa dönüp farklı olasılıkları deneriz. Sorun çözme yaratıcılığımızın devreye girdiği yer burasıdır. Olası bir nedene dair bir fikir bulmamız, bunun doğru olup olmadığını kontrol etmemiz, değilse sorunu açıklayan bir fikir bulana kadar farklı bir fikir bulmamız gerekiyor. Fikirlerimiz boşluktan gelmiyor. İlham almak için şu anda sahip olduğumuz bilgilere bakarız ve ihtiyaç duymamız halinde daha fazlasını toplarız. Aldığımız hata mesajlarını çevrimiçi olarak aramak veya ilgili uygulamaların belgelerine bakmak, neyin hatalı olabileceğine dair yeni olasılıkları hayal etmemize de yardımcı olabilir. Mümkün olduğunda hipotezimizi kullanıcılarımızın çalıştığı üretim ortamı yerine bir test ortamında kontrol etmeliyiz. Bu şekilde, kullanıcılarımızın yaptıklarına müdahale etmekten kaçınırız ve önemli bir şeyi bozma korkusu olmadan etrafa müdahale edebiliriz. Neyi düzeltmeye çalıştığınıza bağlı olarak bu, kodumuzu yeni kurulan bir makinede denememiz, bir test sunucusunu çalıştırmamız, test verilerini kullanmamız vb. anlamına gelebilir. Kurulumun tamamlanması biraz zaman alabilir ancak ekstra güvenlik kesinlikle buna değer. Hatanın belirli bir üretim ortamıyla ilişkili olduğu görülse bile, üretimi değiştirmeden önce sorunu bir test ortamında yeniden oluşturup oluşturamayacağımızı kontrol etmek her zaman iyi bir fikirdir. Aşırı yüklenmiş sunucu örneğimizde sorun donanımdaysa, bunu bir test sunucusuyla kopyalayamayız. Bu durumda, ya hizmetler kullanılmayana kadar beklememiz ya da ikincil bir sunucu açıp hizmetleri oraya taşımamız ve ancak bundan sonra bilgisayardaki sorunun ne olduğuna bakmamız gerekir. Öte yandan, sorun web hizmetlerinin veya yedekleme hizmetinin bazı yapılandırmalarıyla ilgiliyse, sorunu yine de test sunucusunda görebiliriz. Bu nedenle, her zaman hizmetin bir test örneğini kurarak ve üretim örneğine dokunmadan önce sorunun orada tekrarlanıp tekrarlanmadığını kontrol ederek başlarız. Diyelim ki aynı web sitelerini çalıştıran bir test sunucumuz var. Yedeklemeyi başlattığımızda web sitesinin yanıt vermediğini görüyoruz. Bu harika çünkü yeniden üretim durumumuz var ve doğru şekilde hata ayıklayabiliyoruz. Temel nedeni nasıl bulacağız? Olası bir suçlu, çok fazla disk girişi ve çıkışı olabilir. Bu konuda daha fazla bilgi edinmek için **top**'a benzer bir araç olan ve hangi süreçlerin en fazla girdi ve çıktıyı kullandığını görmemizi sağlayan **iotop**'u kullanabiliriz. İlgili diğer araçlar **iostat ve vmstat**'tır; bu araçlar giriş/çıkış işlemleri ve sanal bellek işlemleriyle ilgili istatistikleri gösterir. Sorun, sürecin çok fazla girdi veya çıktı üretmesiyse, yedekleme sistemimizin diske erişim önceliğini azaltmasını ve web hizmetlerinin de bunu kullanmasına izin vermesini sağlamak için **ionice** gibi bir komut kullanabiliriz. Ya giriş ve çıkış sorun değilse? Diğer bir seçenek de, yedeklenecek verileri merkezi bir sunucuya ilettiği ve bu aktarımın diğer her şeyi engellediği için hizmetin çok fazla ağ kullanması olabilir. Bunu, ağ arayüzlerindeki mevcut trafiği gösteren, top benzeri başka bir araç olan **iftop**'u kullanarak kontrol edebiliriz. Yedekleme ağ bant genişliğinin tamamını tüketiyorsa, yedekleme yazılımının belgelerine bakabilir ve bant genişliğini sınırlama seçeneğinin zaten içerip içermediğini kontrol edebiliriz. Genellikle verileri yedeklemek için kullanılan **rsync** komutu, tam da bu amaç için bir **-bwlimit** içerir. Bu seçenek mevcut değilse, kullanılan bant genişliğini sınırlamak için Trickle gibi bir program kullanabiliriz. Peki ya sorun ağda da değilse? Unutmayın, hata ayıklama yaratıcılığımızı uygulamaya koymamız ve başarısızlığın diğer olası nedenlerini bulmamız gerekiyor. Diğer bir seçenek de seçilen sıkıştırma algoritmalarının çok agresif olması ve yedeklerin sıkıştırılmasının sunucunun tüm işlem gücünü kullanması olabilir. Bunu, sıkıştırma seviyesini düşürerek veya **nice** komutunu kullanarak işlemin önceliğini azaltarak ve CPU'ya erişerek çözebiliriz.Eğer durum hâlâ böyle değilse, aramaya devam etmemiz, daha önce gözden kaçırdığımız bir şey bulup bulmadığımızı görmek için günlükleri kontrol etmemiz gerekir. Yedekleme yazılımı ile web'de gezinme yazılımı arasındaki etkileşimlerle ilgili benzer sorunlarla karşılaşan diğer insanları internette arayabilir ve sorunumuza neden olabilecek bir şey bulana kadar bunu yapmaya devam edebiliriz. Bunun çok fazla iş gibi göründüğünü biliyorum, ama genellikle o kadar da kötü değil. Genel olarak, elimizdeki araçları kullanarak, yalnızca birkaç denemeden sonra doğru hipoteze ulaşmak için yeterli bilgiyi bulabiliriz ve deneyim kazandıkça, ilk seferde en olası hipotezi seçme konusunda daha iyi hale geliriz. Sırada, hepimizin yüzleşmek zorunda olduğu, zaman zaman ortaya çıkan zor türde bir teknik sorundan bahsedeceğiz.

When you first come across these concepts, it might seem that once you have a reproduction case, you already know the root cause of the problem. But more often than not, it's not true. In our overloaded server example, we figured out that the backup system was blocking the websites from working, and so we mitigated that immediate problem to unblock the user. But we didn't really look into the root cause of our server being stuck. This could be because the network bandwidth is saturated, the disk transfer is too slow, the hard drive is faulty, or a bunch of other reasons. We also didn't do anything to make sure our backups could run successfully in the future. Understanding the root cause is essential for performing the long-term remediation. So how do we go about finding the actual root cause of the problem? We generally follow a cycle of looking at the information we have, coming up with a hypothesis that could explain the problem, and then testing our hypothesis. If we confirm our theory, we found the root cause. If we don't, then we go back to the beginning and try different possibility. This is where our problem-solving creativity comes into play. We need to come up with an idea of a possible cause, check if it's correct and if not, come up with a different idea until we find one that explains the problem. Our ideas don't come out of a void. To get inspired, we look at information we currently have and gather more if we need. Searching online for the error messages that we get or looking at the documentation of the applications involved can also help us imagine new possibilities of what might be at fault. Whenever possible, we should check our hypothesis in a test environment, instead of the production environment that our users are working with. That way, we avoid interfering with what our users are doing and we can tinker around without fear of breaking something important. Depending on what you're trying to fix, this might mean we have to try our code in a newly installed machine, spinning up a test server, using test data, and so on. It can take some time to get the setup, but the extra safety is definitely worth it. Even when it seems that the error might be related to the specific production environment, it's always a good idea to check if we can reproduce the problem in a test environment before we modify production. In our overloaded server example, if the problem is with the hardware, we wouldn't be able to replicate it with a test server. In that case, we would need to either wait until the services aren't being used or bring up a secondary server, migrate the services there, and only then look at what's wrong with the computer. On the flip side, if the problem is related to some configuration of either the web services or the backup service, we'd still see it in the test server. So we'd always start by setting up a test instance of the service and checking if the problem replicates there before touching the production instance. So say we have a test server running the same websites. When we start the backup, we see that the website stop responding. This is great because we have re-production case, and we can debug it properly. How do we find the root cause? One possible culprit could be too much disk input and output. To get more info on this, we could use iotop, which is a tool similar to top that lets us see which processes are using the most input and output. Other related tools are iostat and vmstat, these tools show statistics on the input/output operations and the virtual memory operations. If the issue is that the process generates too much input or output, we could use a command like ionice to make our backup system reduce its priority to access the disk and let the web services use it too. What if the input and output is not the issue? Another option would be that the service is using too much network because it's transmitting the data to be backed up to a central server and that transmission blocks everything else. We can check this using iftop, yet another tool similar to top that shows the current traffic on the network interfaces. If the backup is eating all the network bandwidth, we could look at the documentation for the backup software and check if it already includes an option to limit the bandwidth. The rsync command, which is often used for backing up data, includes a -bwlimit, just for this purpose. If that option isn't available, we can use a program like Trickle to limit the bandwidth being used. But what if the network isn't the issue either? Remember, we need to put our debugging creativity to work, and come up with other possible reasons for why it's failing. Another option could be that the compression algorithms selected is too aggressive, and compressing the backups is using all of the server's processing power. We could solve this by reducing the compression level or using the nice command to reduce the priority of the process and accessing the CPU. If that's still not the case, we need to keep looking, check the logs to see if we find anything that we missed before. Maybe look online for other people dealing with similar problems related to interactions of the backing up software with the web surfing software, and keep doing this until we come up with something that could be causing our problem. I know this sounds like a lot of work, but it's usually not that bad. In general, by using the tools available to us, we can find enough info to land on the right hypothesis after only a few tries and with experience, we'll get better at picking up the most likely hypothesis the first time around. Up next, we'll talk about a tricky type of technical problem that we all have to face, intermittent issues.