# Crashes in Complex Systems

Şu ana kadar, tek bir bilgisayarla sınırlı olan hataları nasıl teşhis edip düzelteceğimizi konuştuk. Bu, genellikle tek bir kullanıcı tarafından kullanılan bilgisayarlar için yaygın bir durumdur. Ancak birçok farklı hizmeti içeren karmaşık sistemlere girdiğimizde, daha büyük resme bakmamız ve farklı bilgisayarların birbirleriyle etkileşimde bulunmalarını sağlamamız gerekecek. Diyelim ki şirketinizin e-ticaret sitesinden sorumlusunuz. Kullanıcılar tarafından görülen sayfa, son zamanlarda tüm isteklerin yaklaşık %20'sine dahili sunucu hatası ile yanıt vermeye başladı. Peki, ne olup bittiğini nasıl anlarsınız? Tek bir bilgisayarda sorun giderme prensiplerini uygulamak istiyorsunuz, ancak bu sefer daha büyük bir ölçekte. Bu nedenle, hizmeti sağlayan sunuculardaki günlük mesajlarını kontrol etmek isteyeceksiniz ve sorunu neyin tetiklediğine dair ek bilgi bulup bulmadığınızı görmek için. Başarısız olan servise özgü günlüğü bulmak ve ayrıca genel sistem günlüklerine bakmak isteyeceksiniz, çünkü genelde sunucuyu etkileyen bir sorun olup olmadığını görmek önemlidir. Bu örnekte, diyelim ki günlüklerde, "sunucudan geçersiz yanıt" şeklinde bir dizi giriş buldunuz. Bu harika bir hata mesajı değil. Talebin ne olduğunu veya yanıtın ne olduğunu bilmiyorsunuz, ancak en azından ne olup bittiğinin, genel sistem içindeki başka bir hizmetle ilgili olduğuna dair bir ipucu. Bu olayın yakın zamanda başarısız olduğunu söyledik, bu nedenle doğru çalıştığı ve başarısız olduğu arasındaki değişikliği bulmak mantıklı olabilir. Sistemde yeni bir sürüm mü dağıtıldı? İsteklerle ilgili herhangi ilgili değişiklikler oldu mu? Diyelim ki bu Salı sabahı oluyor ve bu servisin en son sürümü geçen hafta yayımlandı. Her şey bugüne kadar iyi çalışıyordu ve istekler normal görünüyordu, olağan dışı bir durum yoktu. Bu durumda, servis kendisi muhtemelen sorunsuzdur, ancak sistemdeki diğer hizmetler nasıl? Veritabanı, kimlik doğrulama servisi veya envanter, fatura veya tedarik sistemleri gibi diğer alt sistemlerden birinin yeni bir sürümü mü oldu? Son değişikliklere baktığınızda, ön yüze ve arka uç servisleri arasında kullanılan yük dengeleyiciye gün içinde bir dizi değişiklik yapıldığını görüyorsunuz. Tek ipucunuz, servisten gelen yanıtın geçersiz olduğu. Bu değişikliklerin hataya neden olup olmadığından emin değilsiniz, ancak şüpheli görünüyorlar. Mümkünse, sorunu oluşturan değişiklikleri geri almanın en iyi strateji olduğunu söyleyebiliriz, hatta bu değişikliğin gerçek nedeni olup olmadığından emin olmasanız bile. Altyapınız geri alma işlemlerine izin veriyorsa, daha fazla soruşturma yapmadan önce bunu deneyin. Neden mi? Çünkü bu şekilde, eğer bu değişiklik sorunun nedeniyse, servisi sağlıklı bir duruma geri getireceksiniz veya geri alma işlemi sorunu çözmüyorsa bu değişikliği olası bir neden olarak ortadan kaldıracaksınız. Geri alma işlemini yapın veya yapmayın, yardımcı olmayan hata mesajlarıyla karşılaştığınızda onları geliştirmek iyi bir fikirdir. Yanıtın sadece geçersiz olduğunu söylemesi yerine, talebin ve yanıtın ne olduğu ve neden yanıtın geçersiz olduğunu içerecek şekilde değiştirin. Böylece, bir sonraki benzer sorunu gidermeye çalıştığınızda daha fazla bilgiye sahip olursunuz. Bu örnekte, eğer hata bu bilgileri içerseydi, geçersiz yanıtın bir 404 hatası olduğunu görecektiniz. Bu, envanter sistemine ait bir parça olarak havuza eklenen bir sunucunun, aslında tedarik sistemine ait olduğu için oluşmuştu. Şimdi, diyelim ki birkaç hafta sonra aynı serviste yine bir dizi dahili sunucu hatası görüyorsunuz. Yine de yük dengeleyicisinin hatası olduğunu varsaymak cazip olabilir, ancak artık günlüklere her zaman ilk olarak bakmanız ve ne bulduğunuza göre hareket etmeniz gerektiğini biliyorsunuz. Bu sefer hata nedeniyle aynı olmamalı. Günlüklere baktığınızda, örneğin, ön yüz sunucularının sadece birinin aslında sorundan etkilendiğini fark edebilirsiniz. Diğer makinelerin içeriği başarıyla sunuyor. Bu durumda, ilk olarak bu hizmeti sağlayabilen sunucu havuzundan bu makineyi çıkarmaya başlarsınız. Bu şekilde, kullanıcıların daha fazla hata almasını önlersiniz. Peki, bozuk makine ile neyin yanlış olduğunu araştırabilirsiniz. Şu ana kadar fark etmiş olabileceğiniz gibi, bu tür karmaşık sistemlerle uğraşırken iyi günlüklere sahip olmak, neyin olduğunu anlamak için esastır. Bunun üzerine, servisin ne yaptığını iyi izlemek ve tüm değişiklikler için sürüm kontrolünü kullanmak isteyeceksiniz, böylece hızlı bir şekilde neyin değiştiğini kontrol edebilir ve gerektiğinde geri alabilirsiniz. Ayrıca, yeni makineleri hızlı bir şekilde dağıtabilmeniz önemlidir. Bu, onları kullanmanız gerektiğinde bekleme durumunda sunucular tutarak veya ihtiyaç olduğunda yeni sunucuları dağıtabilmenizi sağlayan test edilmiş bir iş akışına sahip olarak elde edilebilir. Bugün birçok şirket, hizmetleri buluta çalışan sanal makineler üzerine otomatik süreçlerle dağıtma konusunda otomasyon süreçlerine sahiptir. Bunun kurulması biraz zaman alabilir, ancak bir kez yaptığınızda kullanmakta olduğunuz sunucu sayısını çok kolay bir şekilde artırabilir veya azaltabilirsiniz. Bu, sorunları araştırma ve çözme konusunda oldukça yardımcı olabilir. Ancak sanal makineler olarak çalışan sunucular, özellikle bulutta çalışıyorsa, bu hizmetlere dış sınırların uygulanabileceği bir durumu unutmamak önemlidir. CPU zamanı, RAM veya ağ genişliği gibi kaynaklar yapay olarak sınırlanabilir. Ve sadece bu kadar değil, belirli harici hizmetlerin kullanımı da sınırlanabilir, aynı anda kaç veritabanı bağlantısına sahip olabileceğiniz veya ne kadar veri depolayabileceğiniz gibi. Eğer bu sınırlar uygulanan servislerinizde sorunlara neden oluyorsa, kaynaklarınızı nasıl kullandığınızı tekrar düşünmeniz gerekebilir. Karmaşık bir sistemde sorunla karşılaştığınızda kullanabileceğiniz bir dizi teknikten bahsettik. Mevcut günlüklere bakma, sistem en son çalışırken nelerin değiştiğini anlama, önceki bir duruma geri dönme, hatalı sunucuları havuzdan çıkarma veya ihtiyaca göre yeni sunucuları dağıtma. Bir sonraki adımda, daha büyük olaylarla başa çıkmanın farklı bir yönünü keşfedeceğiz: iletişim ve belgeleme.

Up to now we've talked about how to diagnose and fix errors that are confined to one computer. That's a common case for computers that are used by a single user. But once we start going into complex systems that involve many different services, we'll need to take a look at the bigger picture and have different computers interact with each other. Say you're in charge of the e-commerce site for your company. The page as seen by the users recently started responding with internal server error to about 20% of all requests. How do you figure out what's going on? You want to apply the same principles that we saw for troubleshooting a problem on one computer, but this time at a larger scale. So you'll want to check the log messages in the servers providing the service, and see if you find any additional information pointing to what's causing the issue. You'll want to find any log specific to the service that's failing, and also look at the general system logs to see if there's a problem affecting the server in general. For this example, let's say you find a bunch of entries in the logs that say, invalid response from server. That's not a great error message. You don't know what the request was or what the response was, but it's at least a clue that whatever's happening is related to some other service in the overall system. We said that this started failing recently, so it might make sense to figure out what changed between what it was working correctly and when it started to fail. Was there a new version of the system deployed? Were there any relevant changes regarding the requests? Let's say this is happening on a Tuesday morning, and the latest release of this service was the previous week. Things were working fine until today, and the requests seemed normal, nothing out of the ordinary. So the service itself is probably okay, but what about the other services involved in the system? Was there a new version of one of the underlying systems, like the database, the authentication service, or some other back-end server like the inventory, billing, or procurement systems? Looking at recent changes, you see that there were a bunch of changes made earlier in the day to the load balancer used between the front-end and the back-end services. Since the only clue you have is that the response from the service was invalid, you're not sure that these changes are at fault, but they sure seem suspicious. Whenever possible, the best strategy is to roll back the changes that you suspect are causing the issue, even if you aren't 100% sure if this is the actual cause. If your infrastructure allows easy rollbacks, try that before doing any further investigation. Why? Because that way, you'll restore the service back to health if it was the cause, or you'll eliminate this change as a possible cause if doing the rollback doesn't help. Whether you do the rollback or not, when coming across unhelpful error messages, it's a good idea to improve them. Instead of the error just saying that the response is invalid, change it to include what the request and the response were, and why the response was invalid. That way, the next time you're trying to debug a similar issue you already have more information to work with. For this example, if the error had included this information you'd have seen that the invalid response was a 404 error. This was caused by having a server added to the pool as part of the inventory system, but the server actually belonged to the procurement system. Now, say a couple of weeks later you see that again, there are a bunch of internal server errors in the same service. It might be tempting to assume that it's the load balancer's fault once again, but by now you know that you should always look at the logs first and see what you find. There's no reason why the error should be the same this time. When looking at the logs you may notice, for example, that only one of the front-end servers is actually affected by the problem. All the other machines are serving their content successfully. In a case like this, you'd start by first removing the machine from the pool of servers that can provide this service. That way, you avoid users getting any more errors. Well, you can investigate what's going on with the broken machine. As you've probably realized by now, when dealing with complex systems like these having good logs is essential to understanding what's going on. On top of that, you'll want to have good monitoring of what the service is doing and use version control for all changes so that you can quickly check what's changed and roll back when needed. It's also important that you can very quickly deploy new machines when necessary. This could be achieved by either keeping standby servers, in case you need to use them, or by having a tested pipeline that allows you to deploy new servers on demand. A lot of companies today have automated processes for deploying services to virtual machines running in the cloud. This can take a bit of time to set up, but once you've done that you can very easily increase or reduce the amount of servers you're using. This can help a lot when investigating and solving problems. But one thing to take into account when the servers are running as virtual machines, especially if they're running in the cloud, is that there might be external limits apply to these services. Resources, like the available CPU time, RAM, or network bandwidth, might be artificially capped. And not only that, the use of certain external services can also be limited, like how many database connections you can have at the same time or how much data you can store. If these limits are causing problems with your application, you might need to rethink how you use your resources. We've covered a bunch of techniques that you can use when facing a problem in a complex system. Looking at the available logs, figuring out what changed since the system was last working, rolling back to a previous state, removing faulty servers from the pool, or deploying new servers on demand. Up next, we'll explore a different part of dealing with bigger incidents, communication and documentation.